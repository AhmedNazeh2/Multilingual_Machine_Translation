{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr3ilSoDOMn0"
      },
      "source": [
        "* `Importing Libraries`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXWDC0SSOMn2",
        "outputId": "a96bebbf-49da-4430-9f11-9285af136afd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re, os\n",
        "import random\n",
        "import pandas as pd\n",
        "import string, time, math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "E-l48DufQDZm",
        "outputId": "b0b11d1a-241b-4842-b67c-a32bc1b0df16"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1+cu121'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmCtWh5dZXr6",
        "outputId": "09e61266-1efc-4900-e288-d2300f6d866f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/drive/MyDrive/NMT’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir /content/drive/MyDrive/NMT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoX4Z1C9P1Eh",
        "outputId": "64388c50-313d-4160-e04f-dc2db3e111a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bnh5PMsbOMn4"
      },
      "source": [
        "* `Reading the data`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ae2kAPdlOMn4",
        "outputId": "a0579d69-2b60-4e95-d684-4356a4115b5f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-389e3775-6843-49a3-a3e5-c3236a9d38ee\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Italian</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Ciao!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Ciao.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Corri!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Corra!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Correte!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-389e3775-6843-49a3-a3e5-c3236a9d38ee')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-389e3775-6843-49a3-a3e5-c3236a9d38ee button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-389e3775-6843-49a3-a3e5-c3236a9d38ee');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3e9f1882-190a-4da7-a26e-143d75cacacc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3e9f1882-190a-4da7-a26e-143d75cacacc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3e9f1882-190a-4da7-a26e-143d75cacacc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "  English   Italian\n",
              "0     Hi.     Ciao!\n",
              "1     Hi.     Ciao.\n",
              "2    Run!    Corri!\n",
              "3    Run!    Corra!\n",
              "4    Run!  Correte!"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "FILEPATH = os.path.join('/content/drive/MyDrive/ita.txt')\n",
        "\n",
        "df = pd.read_csv(FILEPATH, sep='\\t', header=None, names=['English', 'Italian', 'License'])\n",
        "\n",
        "df.drop(columns='License', inplace=True)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2s9Ugg5OMn4"
      },
      "outputs": [],
      "source": [
        "df.to_csv(os.path.join('/content/drive/MyDrive/eng-ita.txt'), sep='\\t', index=False, header=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkxfLj_bOMn5"
      },
      "source": [
        "### `Dataset preperation helper functions`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-3kd7SFOMn5"
      },
      "outputs": [],
      "source": [
        "SOS_token = 1\n",
        "EOS_token = 2\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {'<UNK>' : 0}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {1: \"SOS\", 2: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS, EOS and <UNK>\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Db0pRRd1OMn5"
      },
      "outputs": [],
      "source": [
        "def unicodeToAscii(s):\n",
        "    \"\"\"Turn a Unicode string to plain ASCII\"\"\"\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def normalizeString(s):\n",
        "    \"\"\"Lowercase, trim, and remove non-letter characters\"\"\"\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
        "    return s.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkNvriWTOMn5"
      },
      "outputs": [],
      "source": [
        "def readLangs(lang1, lang2, reverse=True):\n",
        "    \"\"\"This function takes in our csv to create input_lang, output_lang, and pairs variables that we will use\n",
        "    to prepare our dataset. This function uses the helper functions to clean the text and normalize the strings\"\"\"\n",
        "\n",
        "    print(\"Reading Lines...\")\n",
        "\n",
        "    # Read file and split into lines\n",
        "    lines = open('/content/drive/MyDrive/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "            read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZKq4gWrOMn5"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 12\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \", \"I don t\", \"Do you\", \"I want\", \"Are you\", \"I have\", \"I think\",\n",
        "    \"I can t\", \"I was\", \"He is\", \"I m not\", \"This is\", \"I just\", \"I didn t\",\n",
        "    \"I am\", \"I thought\", \"I know\", \"Tom is\", \"I had\", \"Did you\", \"Have you\",\n",
        "    \"Can you\", \"He was\", \"You don t\", \"I d like\", \"It was\", \"You should\",\n",
        "    \"Would you\", \"I like\", \"It is\", \"She is\", \"You can t\", \"He has\",\n",
        "    \"What do\", \"If you\", \"I need\", \"No one\", \"You are\", \"You have\",\n",
        "    \"I feel\", \"I really\", \"Why don t\", \"I hope\", \"I will\", \"We have\",\n",
        "    \"You re not\", \"You re very\", \"She was\", \"I love\", \"You must\", \"I can\")\n",
        "\n",
        "# eng_prefixes = (map(lambda x: x.lower(), eng_prefixes))\n",
        "# eng_prefixes = set(eng_prefixes)\n",
        "eng_prefixes = tuple(item.lower() for item in eng_prefixes)\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHxKWBXfOMn6"
      },
      "source": [
        "The full process for preparing the data is:\n",
        "\n",
        "- Read text file and split into lines, split lines into pairs\n",
        "\n",
        "- Normalize text, filter by length and content\n",
        "\n",
        "- Make word lists from sentences in pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyVAfpmVOMn6",
        "outputId": "782f2dc0-4b22-4f5c-8e8a-da95eb74cfca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading Lines...\n",
            "Read 377937 sentence pairs\n",
            "Trimmed to 132757 sentence pairs\n",
            "\n",
            "Counting words...\n",
            "Counted words:\n",
            "ita 12744\n",
            "eng 7099\n",
            "['tu vuoi qualcosa ?', 'do you want something ?']\n"
          ]
        }
      ],
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"\\nCounting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'ita', True)\n",
        "print(random.choice(pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5L5R8R-OMn6"
      },
      "source": [
        "### `Building the seq2seq model`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoTIY5J6OMn6"
      },
      "source": [
        "* `The Encoder`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZ1E3SV0OMn6"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF_o1gdXOMn6"
      },
      "source": [
        "![image-2.png](attachment:image-2.png)\n",
        "- Input Size: Number of unique tokens (vocabulary size).\n",
        "- Hidden Size: Dimensionality of the hidden state (and embedding vectors)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaWyXG1MOMn6"
      },
      "source": [
        "* `The Decoder`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkbAEPqBOMn6"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long,\n",
        "                            device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1) # This gets the token with the highest probability (argmax) from the decoder's output at the current time step\n",
        "                decoder_input = topi.squeeze(-1).detach() # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        return decoder_outputs, decoder_hidden, None  # We return `None` for consistency in the training loop\n",
        "\n",
        "    def forward_step(self, input, hidden):\n",
        "        output = self.embedding(input)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.out(output)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![image-2.png](attachment:image-2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8lB-16nOMn6"
      },
      "source": [
        "* `Attention Decoder`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-Bl_ePQOMn6"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "\n",
        "        return context, weights\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        embedded =  self.dropout(self.embedding(input))\n",
        "\n",
        "        query = hidden.permute(1, 0, 2)\n",
        "        context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        input_gru = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "        output, hidden = self.gru(input_gru, hidden)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, hidden, attn_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3NtfyhiOMn7"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRDqixCkOMn7"
      },
      "source": [
        "---\n",
        "### Training\n",
        "* `Preparing Training Data`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuYALBXFOMn7"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index.get(word, lang.word2index['<UNK>']) for word in sentence.strip().split() if word]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    print('tensorFromPair', input_tensor)\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "def get_dataloader(batch_size):\n",
        "    input_lang, output_lang, pairs = prepareData('eng', 'ita', True)\n",
        "\n",
        "    n = len(pairs)\n",
        "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "\n",
        "    for idx, (inp, tgt) in enumerate(pairs):\n",
        "        inp_ids = indexesFromSentence(input_lang, inp)\n",
        "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
        "        inp_ids.append(EOS_token)\n",
        "        tgt_ids.append(EOS_token)\n",
        "        # Print for debugging\n",
        "        # print(f\"inp_ids type: {type(inp_ids)}, content: {inp_ids}\")\n",
        "        # print(f\"tgt_ids type: {type(tgt_ids)}, content: {tgt_ids}\")\n",
        "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "\n",
        "\n",
        "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
        "                            torch.LongTensor(target_ids).to(device))\n",
        "\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "    return input_lang, output_lang, train_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI07NcjpOMn7"
      },
      "source": [
        "* `Training the Model`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Az53ZmBOMn7"
      },
      "outputs": [],
      "source": [
        "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
        "        decoder_optimizer, criterion):\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in dataloader:\n",
        "        input_tensor, target_tensor = data\n",
        "\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "        loss = criterion(\n",
        "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "            target_tensor.view(-1)\n",
        "        )\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n27EZuW9OMn7"
      },
      "outputs": [],
      "source": [
        "# This is a helper function to print time elapsed and estimated time remaining given the current time and progress %.\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvrx5DmzOMn7"
      },
      "source": [
        "---\n",
        "The whole training process looks like this:\n",
        "\n",
        "- Start a timer\n",
        "\n",
        "- Initialize optimizers and criterion\n",
        "\n",
        "- Create set of training pairs\n",
        "\n",
        "- Start empty losses array for plotting <br>\n",
        "<br>Then we call `train` many times and occasionally print the progress (% of examples, time so far, estimated time) and average loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPtrVBeQOMn7"
      },
      "outputs": [],
      "source": [
        "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001, checkpoint_path=None,\n",
        "            print_every=1, plot_every=5, save_every=10, checkpoint_dir='/content/drive/MyDrive/NMT'):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    # Create checkpoint directory if it doesn't exist\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    start_epoch = 1\n",
        "    if checkpoint_path:\n",
        "        start_epoch, _ = load_checkpoint(checkpoint_path, encoder, decoder)\n",
        "        print(f\"Resuming training from epoch {start_epoch}\")\n",
        "\n",
        "    for epoch in range(start_epoch, n_epochs + 1):\n",
        "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
        "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "        if epoch % save_every == 0:\n",
        "            checkpoint = {\n",
        "                'epoch': epoch,\n",
        "                'encoder_state_dict': encoder.state_dict(),\n",
        "                'decoder_state_dict': decoder.state_dict(),\n",
        "                'encoder_optimizer': encoder_optimizer.state_dict(),\n",
        "                'decoder_optimizer': decoder_optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "            }\n",
        "            torch.save(checkpoint, f\"/content/drive/MyDrive/NMT/checkpoint_epoch_{epoch}.pt\")\n",
        "            print(f\"Checkpoint saved at epoch {epoch}\")\n",
        "\n",
        "    showPlot(plot_losses)\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "    print('joo')\n",
        "\n",
        "def load_checkpoint(checkpoint_path, encoder, decoder):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "    decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
        "    return checkpoint['epoch'], checkpoint['loss']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5ii0VVYOMn7"
      },
      "source": [
        "---\n",
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_A0ukH1OMn8"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_words = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            decoded_words.append(output_lang.index2word[idx.item()])\n",
        "    return decoded_words, decoder_attn\n",
        "\n",
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLKOXXFZOMn8"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tzr6YXWOMn8"
      },
      "source": [
        "### Training and Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjW97_Q6OMn8",
        "outputId": "36c0ebaf-270b-46fa-f1e5-cd13c3440a0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading Lines...\n",
            "Read 377937 sentence pairs\n",
            "Trimmed to 132757 sentence pairs\n",
            "\n",
            "Counting words...\n",
            "Counted words:\n",
            "ita 12744\n",
            "eng 7099\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-f23a1eb95fc7>:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resuming training from epoch 50\n",
            "1m 7s (- 1m 7s) (50 50%) 0.1689\n",
            "Checkpoint saved at epoch 50\n",
            "2m 10s (- 2m 5s) (51 51%) 0.0888\n",
            "3m 10s (- 2m 56s) (52 52%) 0.0800\n",
            "4m 11s (- 3m 43s) (53 53%) 0.0764\n",
            "5m 12s (- 4m 25s) (54 54%) 0.0741\n",
            "6m 12s (- 5m 5s) (55 55%) 0.0713\n",
            "7m 13s (- 5m 40s) (56 56%) 0.0698\n",
            "8m 14s (- 6m 13s) (57 56%) 0.0676\n",
            "9m 14s (- 6m 41s) (58 57%) 0.0664\n",
            "10m 15s (- 7m 7s) (59 59%) 0.0658\n",
            "11m 16s (- 7m 30s) (60 60%) 0.0646\n",
            "Checkpoint saved at epoch 60\n",
            "12m 17s (- 7m 51s) (61 61%) 0.0642\n",
            "13m 18s (- 8m 9s) (62 62%) 0.0636\n",
            "14m 18s (- 8m 24s) (63 63%) 0.0628\n",
            "15m 19s (- 8m 37s) (64 64%) 0.0624\n",
            "16m 19s (- 8m 47s) (65 65%) 0.0613\n",
            "17m 20s (- 8m 55s) (66 66%) 0.0620\n",
            "18m 20s (- 9m 2s) (67 67%) 0.0610\n",
            "19m 21s (- 9m 6s) (68 68%) 0.0604\n",
            "20m 21s (- 9m 8s) (69 69%) 0.0597\n",
            "21m 22s (- 9m 9s) (70 70%) 0.0601\n",
            "Checkpoint saved at epoch 70\n",
            "22m 23s (- 9m 8s) (71 71%) 0.0595\n",
            "23m 23s (- 9m 5s) (72 72%) 0.0593\n",
            "24m 24s (- 9m 1s) (73 73%) 0.0589\n",
            "25m 25s (- 8m 55s) (74 74%) 0.0584\n",
            "26m 25s (- 8m 48s) (75 75%) 0.0587\n",
            "27m 26s (- 8m 39s) (76 76%) 0.0591\n",
            "28m 26s (- 8m 29s) (77 77%) 0.0582\n",
            "29m 28s (- 8m 18s) (78 78%) 0.0577\n",
            "30m 28s (- 8m 6s) (79 79%) 0.0584\n",
            "31m 28s (- 7m 52s) (80 80%) 0.0579\n",
            "Checkpoint saved at epoch 80\n",
            "32m 30s (- 7m 37s) (81 81%) 0.0574\n",
            "33m 32s (- 7m 21s) (82 82%) 0.0575\n",
            "34m 31s (- 7m 4s) (83 83%) 0.0576\n",
            "35m 32s (- 6m 46s) (84 84%) 0.0575\n",
            "36m 34s (- 6m 27s) (85 85%) 0.0569\n",
            "37m 33s (- 6m 6s) (86 86%) 0.0567\n",
            "38m 34s (- 5m 45s) (87 87%) 0.0567\n",
            "39m 35s (- 5m 23s) (88 88%) 0.0569\n",
            "40m 35s (- 5m 1s) (89 89%) 0.0575\n",
            "41m 35s (- 4m 37s) (90 90%) 0.0564\n",
            "Checkpoint saved at epoch 90\n",
            "42m 37s (- 4m 12s) (91 91%) 0.0572\n",
            "43m 36s (- 3m 47s) (92 92%) 0.0568\n",
            "44m 38s (- 3m 21s) (93 93%) 0.0570\n",
            "45m 38s (- 2m 54s) (94 94%) 0.0567\n",
            "46m 38s (- 2m 27s) (95 95%) 0.0567\n",
            "47m 38s (- 1m 59s) (96 96%) 0.0562\n",
            "48m 39s (- 1m 30s) (97 97%) 0.0555\n",
            "49m 40s (- 1m 0s) (98 98%) 0.0561\n",
            "50m 40s (- 0m 30s) (99 99%) 0.0562\n",
            "51m 40s (- 0m 0s) (100 100%) 0.0560\n",
            "Checkpoint saved at epoch 100\n",
            "joo\n"
          ]
        }
      ],
      "source": [
        "hidden_size = 256\n",
        "batch_size = 64\n",
        "\n",
        "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "train(train_dataloader, encoder, decoder, 100, print_every=1, plot_every=5,\n",
        "      checkpoint_path='/content/drive/MyDrive/NMT/checkpoint_epoch_50.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsbpaQd_OMn8",
        "outputId": "3f0b761a-99f7-4304-b4af-04ddcff249c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> sei un amico di tom ?\n",
            "= are you a friend of tom s ?\n",
            "< are you a friend of tom s ? <EOS>\n",
            "\n",
            "> voglio che mi ascolti\n",
            "= i want you to listen to me\n",
            "< <EOS>\n",
            "\n",
            "> dovresti andare li\n",
            "= you should go there\n",
            "< you should go there <EOS>\n",
            "\n",
            "> siete d accordo ?\n",
            "= do you agree ?\n",
            "< do you agree ? <EOS>\n",
            "\n",
            "> riuscite a iniziare domani ?\n",
            "= can you start tomorrow ?\n",
            "< can you start tomorrow ? <EOS>\n",
            "\n",
            "> siete sicuri che tom stia venendo ?\n",
            "= are you sure tom is coming ?\n",
            "< are you sure tom is coming ? <EOS>\n",
            "\n",
            "> mi mancano tre anni alla pensione\n",
            "= i have three years left until retirement\n",
            "< <EOS>\n",
            "\n",
            "> restero a casa\n",
            "= i will stay at home\n",
            "< <EOS>\n",
            "\n",
            "> devi andare a casa\n",
            "= you must go home\n",
            "< you must go home <EOS>\n",
            "\n",
            "> e la loro unica scelta\n",
            "= it is their only choice\n",
            "< <EOS>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "encoder.eval()\n",
        "decoder.eval()\n",
        "evaluateRandomly(encoder, decoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5a_Ln_MOMn9"
      },
      "source": [
        "### Visualizing Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ3h6rx6OMn9",
        "outputId": "b179ce86-05a1-49ce-b729-5f6ceb890778"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input = Sono pronto ad andare.\n",
            "output = <EOS>\n",
            "input = Penso che abbiamo bisogno di più acqua.\n",
            "output = <EOS>\n",
            "input = Ti sei assicurato che la porta sia chiusa a chiave?\n",
            "output = it is both punished who showed me the door is <EOS>\n",
            "input = Il tuo nuovo vestito ti sta molto bene addosso.\n",
            "output = you re going to have your new time you will promise your\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-21-cda917074d4b>:8: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
            "<ipython-input-21-cda917074d4b>:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_yticklabels([''] + output_words)\n"
          ]
        }
      ],
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    ax = fig.add_subplot(111)\n",
        "    \n",
        "    # Plot attention weights\n",
        "    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticks(range(len(input_sentence.split(' ')) + 1))\n",
        "    ax.set_yticks(range(len(output_words) + 1))\n",
        "\n",
        "    # Set tick labels\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Ensure labels are shown at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    # Assume `evaluate` returns a tuple (output_words, attentions)\n",
        "    output_words, attentions = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
        "    \n",
        "    print(f\"input = {input_sentence}\")\n",
        "    print(f\"output = {' '.join(output_words)}\")\n",
        "    \n",
        "    # Display attention visualization\n",
        "    showAttention(input_sentence, output_words, attentions[0, :len(output_words), :])\n",
        "\n",
        "\n",
        "evaluateAndShowAttention('Sono pronto ad andare.')\n",
        "\n",
        "evaluateAndShowAttention('Penso che abbiamo bisogno di più acqua.')\n",
        "\n",
        "evaluateAndShowAttention('Ti sei assicurato che la porta sia chiusa a chiave?')\n",
        "\n",
        "evaluateAndShowAttention('Il tuo nuovo vestito ti sta molto bene addosso.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJOO4O3X2Ozs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "depi2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
